{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "import nltk\n",
    "import pickle\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from pycocotools.coco import COCO\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(json, threshold):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    coco = COCO(json)\n",
    "    counter = Counter()\n",
    "    ids = coco.anns.keys()\n",
    "    for i, id in enumerate(ids):\n",
    "        caption = str(coco.anns[id]['caption'])\n",
    "        tokens = nltk.tokenize.word_tokenize(caption.lower())\n",
    "        counter.update(tokens)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(\"[%d/%d] Tokenized the captions.\" %(i, len(ids)))\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Creates a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Adds the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 9956\n",
      "Saved the vocabulary wrapper to './pytorch-tutorial/tutorials/03-advanced/image_captioning/data/vocab.pkl'\n"
     ]
    }
   ],
   "source": [
    "vocab_path='./pytorch-tutorial/tutorials/03-advanced/image_captioning/data/vocab.pkl'\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: %d\" %len(vocab))\n",
    "print(\"Saved the vocabulary wrapper to '%s'\" %vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.38s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/414113] Tokenized the captions.\n",
      "[1000/414113] Tokenized the captions.\n",
      "[2000/414113] Tokenized the captions.\n",
      "[3000/414113] Tokenized the captions.\n",
      "[4000/414113] Tokenized the captions.\n",
      "[5000/414113] Tokenized the captions.\n",
      "[6000/414113] Tokenized the captions.\n",
      "......"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[206000/414113] Tokenized the captions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[411000/414113] Tokenized the captions.\n",
      "[412000/414113] Tokenized the captions.\n",
      "[413000/414113] Tokenized the captions.\n",
      "[414000/414113] Tokenized the captions.\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(json='./pytorch-tutorial/tutorials/03-advanced/image_captioning/data/annotations/captions_train2014.json',\n",
    "                        threshold=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_image(image, size):\n",
    "    \"\"\"Resize an image to the given size.\"\"\"\n",
    "    return image.resize(size, Image.ANTIALIAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_images(image_dir, output_dir, size):\n",
    "    \"\"\"Resize the images in 'image_dir' and save into 'output_dir'.\"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    images = os.listdir(image_dir)\n",
    "    num_images = len(images)\n",
    "    for i, image in enumerate(images):\n",
    "        with open(os.path.join(image_dir, image), 'r+b') as f:\n",
    "            with Image.open(f) as img:\n",
    "                img = resize_image(img, size)\n",
    "                img.save(os.path.join(output_dir, image), img.format)\n",
    "        if i % 100 == 0:\n",
    "            print (\"[%d/%d] Resized the images and saved into '%s'.\"\n",
    "                   %(i, num_images, output_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/82783] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/train_resized2014'.\n",
      "[100/82783] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/train_resized2014'.\n",
      "[200/82783] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/train_resized2014'.\n",
      "......\n"
      "[0/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n",
      "[100/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n",
      "[200/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n",
      "[300/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......\n",
      "[40100/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n",
      "[40200/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n",
      "[40300/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n",
      "[40400/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n",
      "[40500/40504] Resized the images and saved into 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/val_resized2014'.\n"
     ]
    }
   ],
   "source": [
    "splits = ['train', 'val']\n",
    "for split in splits:\n",
    "    image_dir = 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/'+split+'2014'\n",
    "    output_dir = 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/'+split+'_resized2014'\n",
    "    image_size = [256, 256]\n",
    "    resize_images(image_dir, output_dir, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CocoDataset(data.Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "    def __init__(self, root, json, vocab, transform=None):\n",
    "        \"\"\"Set the path for images, captions and vocabulary wrapper.\n",
    "        \n",
    "        Args:\n",
    "            root: image directory.\n",
    "            json: coco annotation file path.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.coco = COCO(json)\n",
    "        self.ids = list(self.coco.anns.keys())\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (image and caption).\"\"\"\n",
    "        coco = self.coco\n",
    "        vocab = self.vocab\n",
    "        ann_id = self.ids[index]\n",
    "        caption = coco.anns[ann_id]['caption']\n",
    "        img_id = coco.anns[ann_id]['image_id']\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        image = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    \n",
    "    We should build custom collate_fn rather than using default collate_fn, \n",
    "    because merging caption (including padding) is not supported in default.\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths\n",
    "\n",
    "\n",
    "def get_loader(root, json, vocab, transform, batch_size, shuffle, num_workers):\n",
    "    \"\"\"Returns torch.utils.data.DataLoader for custom coco dataset.\"\"\"\n",
    "    # COCO caption dataset\n",
    "    coco = CocoDataset(root=root,\n",
    "                       json=json,\n",
    "                       vocab=vocab,\n",
    "                       transform=transform)\n",
    "    \n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for every iteration.\n",
    "    # images: tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: tensor of shape (batch_size, padded_length).\n",
    "    # lengths: list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=coco, \n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=shuffle,\n",
    "                                              num_workers=num_workers,\n",
    "                                              collate_fn=collate_fn)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize the weights.\"\"\"\n",
    "        self.linear.weight.data.normal_(0.0, 0.02)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract the image feature vectors.\"\"\"\n",
    "        features = self.resnet(images)\n",
    "        features = Variable(features.data)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Samples captions for given image features (Greedy search).\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(20):                                      # maximum sampling length\n",
    "            hiddens, states = self.lstm(inputs, states)          # (batch_size, 1, hidden_size), \n",
    "            outputs = self.linear(hiddens.squeeze(1))            # (batch_size, vocab_size)\n",
    "            predicted = outputs.max(1)[1]\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)\n",
    "            inputs = inputs.unsqueeze(1)                         # (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.cat(sampled_ids, 1)                  # (batch_size, 20)\n",
    "        return sampled_ids.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_var(x, volatile=False):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cuda()\n",
    "    return Variable(x, volatile=volatile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('pytorch-tutorial/tutorials/03-advanced/image_captioning/models/'):\n",
    "    os.makedirs('pytorch-tutorial/tutorials/03-advanced/image_captioning/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([ \n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('pytorch-tutorial/tutorials/03-advanced/image_captioning/data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.18s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "img_path='pytorch-tutorial/tutorials/03-advanced/image_captioning/data/train_resized2014'\n",
    "caption_path='pytorch-tutorial/tutorials/03-advanced/image_captioning/data/annotations/captions_train2014.json'\n",
    "batch_size=128\n",
    "data_loader = get_loader(img_path,caption_path, vocab, \n",
    "                             transform, batch_size,\n",
    "                             shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size=256\n",
    "hidden_size=512\n",
    "num_layers=1\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_step=10\n",
    "save_step=1000\n",
    "embed_size=256\n",
    "hidden_size=512\n",
    "num_layers=1\n",
    "num_epochs=5\n",
    "batch_size=128\n",
    "num_workers=2\n",
    "learning_rate=0.001\n",
    "model_path='pytorch-tutorial/tutorials/03-advanced/image_captioning/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_step = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5], Step [0/3236], Loss: 9.1974, Perplexity: 9871.5627\n",
      "Epoch [0/5], Step [10/3236], Loss: 6.6615, Perplexity: 781.6876\n",
      "Epoch [0/5], Step [20/3236], Loss: 5.4381, Perplexity: 230.0063\n",
      "Epoch [0/5], Step [30/3236], Loss: 5.1369, Perplexity: 170.1890\n",
      "Epoch [0/5], Step [40/3236], Loss: 4.7731, Perplexity: 118.2828\n",
      "Epoch [0/5], Step [50/3236], Loss: 4.5814, Perplexity: 97.6540\n",
      "Epoch [0/5], Step [60/3236], Loss: 4.5598, Perplexity: 95.5642\n",
      "Epoch [0/5], Step [70/3236], Loss: 4.5302, Perplexity: 92.7799\n",
      "Epoch [0/5], Step [80/3236], Loss: 4.3768, Perplexity: 79.5837\n",
      "Epoch [0/5], Step [90/3236], Loss: 4.2877, Perplexity: 72.7971\n",
      "Epoch [0/5], Step [100/3236], Loss: 4.1852, Perplexity: 65.7062\n",
      "Epoch [0/5], Step [110/3236], Loss: 4.1310, Perplexity: 62.2426\n",
      "Epoch [0/5], Step [120/3236], Loss: 4.1538, Perplexity: 63.6772\n",
      "Epoch [0/5], Step [130/3236], Loss: 3.8402, Perplexity: 46.5370\n",
      "Epoch [0/5], Step [140/3236], Loss: 3.9060, Perplexity: 49.7013\n",
      "Epoch [0/5], Step [150/3236], Loss: 3.7705, Perplexity: 43.4017\n",
      "Epoch [0/5], Step [160/3236], Loss: 3.5852, Perplexity: 36.0602\n",
      "Epoch [0/5], Step [170/3236], Loss: 3.6954, Perplexity: 40.2602\n",
      "Epoch [0/5], Step [180/3236], Loss: 3.8275, Perplexity: 45.9471\n",
      "......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [1870/3236], Loss: 1.8695, Perplexity: 6.4850\n",
      "Epoch [3/5], Step [1880/3236], Loss: 1.9073, Perplexity: 6.7350\n",
      "......\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Step [3160/3236], Loss: 2.0365, Perplexity: 7.6640\n",
      "Epoch [3/5], Step [3170/3236], Loss: 1.8792, Perplexity: 6.5481\n",
      "Epoch [3/5], Step [3180/3236], Loss: 1.8888, Perplexity: 6.6114\n",
      "Epoch [3/5], Step [3190/3236], Loss: 1.9623, Perplexity: 7.1156\n",
      "Epoch [3/5], Step [3200/3236], Loss: 1.9113, Perplexity: 6.7616\n",
      "Epoch [3/5], Step [3210/3236], Loss: 2.0712, Perplexity: 7.9342\n",
      "Epoch [3/5], Step [3220/3236], Loss: 1.9748, Perplexity: 7.2049\n",
      "Epoch [3/5], Step [3230/3236], Loss: 1.8391, Perplexity: 6.2906\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/zhihao/miniconda2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n  File \"/home/zhihao/miniconda2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n  File \"<ipython-input-6-7e1e986e5b47>\", line 27, in __getitem__\n    image = Image.open(os.path.join(self.root, path)).convert('RGB')\n  File \"/home/zhihao/miniconda2/lib/python3.6/site-packages/PIL/Image.py\", line 2519, in open\nOSError: cannot identify image file 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/train_resized2014/COCO_train2014_000000510671.jpg'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-2fc31e75729e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;31m# Set mini-batch dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m~/miniconda2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Traceback (most recent call last):\n  File \"/home/zhihao/miniconda2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n  File \"/home/zhihao/miniconda2/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 40, in <listcomp>\n  File \"<ipython-input-6-7e1e986e5b47>\", line 27, in __getitem__\n    image = Image.open(os.path.join(self.root, path)).convert('RGB')\n  File \"/home/zhihao/miniconda2/lib/python3.6/site-packages/PIL/Image.py\", line 2519, in open\nOSError: cannot identify image file 'pytorch-tutorial/tutorials/03-advanced/image_captioning/data/train_resized2014/COCO_train2014_000000510671.jpg'\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "            \n",
    "            # Set mini-batch dataset\n",
    "            images = to_var(images, volatile=True)\n",
    "            captions = to_var(captions)\n",
    "            targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "            # Forward, Backward and Optimize\n",
    "            decoder.zero_grad()\n",
    "            encoder.zero_grad()\n",
    "            features = encoder(images)\n",
    "            outputs = decoder(features, captions, lengths)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Print log info\n",
    "            if i % log_step == 0:\n",
    "                print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f'\n",
    "                      %(epoch, num_epochs, i, total_step, \n",
    "                        loss.data[0], np.exp(loss.data[0]))) \n",
    "                \n",
    "            # Save the models\n",
    "            if (i+1) % save_step == 0:\n",
    "                torch.save(decoder.state_dict(), \n",
    "                           os.path.join(model_path, \n",
    "                                        'decoder-%d-%d.pkl' %(epoch+1, i+1)))\n",
    "                torch.save(encoder.state_dict(), \n",
    "                           os.path.join(model_path, \n",
    "                                        'encoder-%d-%d.pkl' %(epoch+1, i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
